{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqlRt5F9y72s",
    "outputId": "69c14206-e4cd-413a-dd59-b6ac2c692d98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2aab881483b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, Subset, default_collate\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, v2\n",
    "from torchvision import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QMjVf_W4n8d",
    "outputId": "326ec86b-97eb-4b50-9733-4335669f08af"
   },
   "outputs": [],
   "source": [
    "# load fashion MNIST data and transform images\n",
    "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zBoNWb2-4o_i"
   },
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zWYuhUbme7Mk"
   },
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape # n=num_images, c=image dimension\n",
    "\n",
    "    # ensure input images are squares\n",
    "    assert h==w, \"image must be square\"\n",
    "\n",
    "    # instantiate patches as a 3D zero tensor (** means power of)\n",
    "    patches = torch.zeros(n, n_patches**2, h*w*c // n_patches**2) # (num_images, num_patches, patch dimension)\n",
    "    patch_size = h //n_patches\n",
    "\n",
    "    for index, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "          for j in range(n_patches):\n",
    "              patch = image[:, i*patch_size: (i+1)*patch_size, j*patch_size: (j+1)*patch_size]\n",
    "              patches[index, i*n_patches + j] = patch.flatten()\n",
    "    return patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Uhs3F9mwkxR_"
   },
   "outputs": [],
   "source": [
    "def get_positional_embedding(seq_length, dim):\n",
    "    pe = torch.ones(seq_length, dim)\n",
    "    for i in range(seq_length):\n",
    "        for j in range(dim):\n",
    "            if j % 2 == 0:\n",
    "                pe[i][j] = np.sin(i/(10000 ** (j/dim)))\n",
    "            elif j % 2 == 1:\n",
    "                pe[i][j] = np.cos(i / (10000 ** ((j-1)/dim)))\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nDU0_vWLnEU9"
   },
   "outputs": [],
   "source": [
    "class MultiHeadSA(nn.Module):\n",
    "    def __init__(self, dim, n_heads=2):\n",
    "        super(MultiHeadSA, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert dim % n_heads == 0, f\"Can't divide dimension {dim} into {n_heads} heads\"\n",
    "\n",
    "        # creating weight matrix of q, k and v\n",
    "        d_head = int(dim / n_heads)\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for x in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for x in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for x in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "\n",
    "        # concat attention from each head together\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PYlurJfonvkg"
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, mlp_ratio=4):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # self-attention layer\n",
    "        self.mhsa = MultiHeadSA(hidden_dim, n_heads)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # feed forward layer\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mlp_ratio * hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mhsa_out = x + self.mhsa(self.norm1(x))\n",
    "        ff_out = mhsa_out + self.mlp(self.norm2(mhsa_out))\n",
    "        return ff_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JlKf18sHe3Cg"
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, chw, n_patches, hidden_dim, n_encodelayers, n_heads, output_dim):\n",
    "    super(VisionTransformer, self).__init__()\n",
    "\n",
    "    self.chw = chw\n",
    "    self.n_patches = n_patches\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_encodelayers = n_encodelayers\n",
    "    self.n_heads = n_heads\n",
    "    self.output_dim = output_dim\n",
    "\n",
    "    # ensure that height and width are divisble by num of patches\n",
    "    assert chw[1] % n_patches == 0, \"Input shape is not divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape is not divisible by number of patches\"\n",
    "    self.patch_size = (chw[1]/n_patches, chw[2]/n_patches)\n",
    "\n",
    "    # linear mapping\n",
    "    self.input_dim = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "    self.linear_map = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "\n",
    "    # classification head\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_dim))\n",
    "\n",
    "    # positional encoding\n",
    "    self.pos_embed = nn.Parameter(torch.tensor(get_positional_embedding(self.n_patches ** 2 + 1, self.hidden_dim)))\n",
    "    # self.pos_embed.requires_grad = False\n",
    "\n",
    "    # transformer encoder\n",
    "    self.encoder_layers = nn.ModuleList([ResidualConnection(hidden_dim, n_heads) for x in range(n_encodelayers)])\n",
    "\n",
    "    # extract classification token\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(self.hidden_dim, output_dim),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, input_image):\n",
    "    patches = patchify(input_image, self.n_patches)\n",
    "    tokens = self.linear_map(patches)\n",
    "\n",
    "    # add classification head to each token (learnable embedding)\n",
    "    tokens_with_class = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "    # add positional embedding\n",
    "    pos_embed = self.pos_embed.repeat(tokens_with_class.shape[0], 1, 1)\n",
    "    pos_embed_token = tokens_with_class + pos_embed\n",
    "\n",
    "    # transformer encoding layers\n",
    "    for layer in self.encoder_layers:\n",
    "      output = layer(pos_embed_token)\n",
    "\n",
    "    # extract classification token\n",
    "    output = output[:, 0]\n",
    "    pred = self.mlp(output)\n",
    "\n",
    "    return pred # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mixup and cutmix functions\n",
    "def mixup(imgs, labels, alpha):\n",
    "    lam = np.random.beta(alpha,alpha)\n",
    "    index = torch.randperm(len(imgs))\n",
    "    shuffled_imgs = imgs[index]\n",
    "    shuffled_labels = labels[index]\n",
    "    new_imgs = lam*imgs + (1-lam)*shuffled_imgs\n",
    "\n",
    "    return new_imgs, shuffled_labels, lam \n",
    "\n",
    "# random bounding box used in cutmix for cut and paste operation\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(data, target, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_target = target[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    new_data = data.clone()\n",
    "    new_data[:, :, bby1:bby2, bbx1:bbx2] = data[indices, :, bby1:bby2, bbx1:bbx2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "\n",
    "    return new_data, shuffled_target, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify train loop to include data augmentation\n",
    "def train_loop(train_loader, model, loss_fn, optimizer, epoch, cutmixalpha, mixupalpha):\n",
    "    train_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # execute data augmentation randomly\n",
    "        if np.random.rand() > 0.5:  # Adjust the probability as needed\n",
    "            augmented_x, y, lam = cutmix(x, y, cutmixalpha)\n",
    "        else:\n",
    "            augmented_x, y, lam = mixup(x, y, mixupalpha)\n",
    "        augmented_x, y = augmented_x.to(device), y.to(device)\n",
    "        \n",
    "        output = model(augmented_x).to(device)\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        train_loss += (loss.item() / len(train_loader))\n",
    "        correct += torch.sum(torch.argmax(output, dim=1) == y).item()\n",
    "        total += len(augmented_x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} loss: {train_loss:.2f}\")\n",
    "    print(f\"Validation accuracy: {correct / total * 100:.2f}%\")\n",
    "    \n",
    "def test_loop(test_loader, model, loss_fn):\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x).to(device)\n",
    "            loss = loss_fn(output, y)\n",
    "            test_loss += (loss.item() / len(test_loader))\n",
    "\n",
    "            correct += torch.sum(torch.argmax(output, dim=1) == y).item()\n",
    "            total += len(x)\n",
    "        test_acc = correct / total * 100\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "batch_size = 512\n",
    "train_dataloader = DataLoader(training_data, shuffle=True, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_data, shuffle=True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey Quah\\AppData\\Local\\Temp\\ipykernel_2404\\1295966561.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.pos_embed = nn.Parameter(torch.tensor(get_positional_embedding(self.n_patches ** 2 + 1, self.hidden_dim)))\n",
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 2.28\n",
      "Validation accuracy: 13.86%\n",
      "Time taken for Epoch 1: 1796.95458984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 2.25\n",
      "Validation accuracy: 18.14%\n",
      "Time taken for Epoch 2: 1886.6079955101013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 2.21\n",
      "Validation accuracy: 22.87%\n",
      "Time taken for Epoch 3: 1841.6621141433716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 2.20\n",
      "Validation accuracy: 24.82%\n",
      "Time taken for Epoch 4: 1706.9671611785889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 2.18\n",
      "Validation accuracy: 27.69%\n",
      "Time taken for Epoch 5: 1873.5339818000793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 2.17\n",
      "Validation accuracy: 27.77%\n",
      "Time taken for Epoch 6: 2127.2694861888885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 2.14\n",
      "Validation accuracy: 31.55%\n",
      "Time taken for Epoch 7: 2075.6422147750854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 2.14\n",
      "Validation accuracy: 31.35%\n",
      "Time taken for Epoch 8: 1759.2664937973022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 2.17\n",
      "Validation accuracy: 28.36%\n",
      "Time taken for Epoch 9: 1914.825074672699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 2.15\n",
      "Validation accuracy: 30.52%\n",
      "Time taken for Epoch 10: 1923.1636435985565\n",
      "Total training time = 18905.892755508423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████| 20/20 [04:33<00:00, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.81\n",
      "Test accuracy: 65.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "chw = (1, 64, 64) # image dimensions\n",
    "output_dim = 10 # Fashion MNIST has 10 classes\n",
    "n_patches = 16\n",
    "\n",
    "# define augmentation alphas\n",
    "cutmixalpha = 1.0\n",
    "mixupalpha = 0.2\n",
    "\n",
    "# optimal parameters\n",
    "hidden_dim = 8 # number of features in each patch's representation\n",
    "n_encodelayers = 8\n",
    "n_heads = 4 # no of attention heads\n",
    "\n",
    "learning_rate = 0.005\n",
    "batch_size = 512\n",
    "num_epochs = 10\n",
    "\n",
    "# instantiate model\n",
    "model = VisionTransformer(chw, n_patches, hidden_dim, n_encodelayers, n_heads, output_dim)\n",
    "\n",
    "# instantiate loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train model with training data\n",
    "time_taken = []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, epoch, cutmixalpha, mixupalpha)\n",
    "    end_time = time.time()\n",
    "    print(f'Time taken for Epoch {epoch+1}: {end_time - start_time}')\n",
    "    time_taken.append(end_time - start_time)\n",
    "    torch.save(model, f'./augmentation_model.pt')\n",
    "\n",
    "# obtain training time\n",
    "total_time = sum(time_taken)\n",
    "print(f'Total training time = {total_time}')\n",
    "\n",
    "# test model to get test loss and accuracy\n",
    "test_accuracy, test_loss = test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Time: 315.09821259180706 minutes\n",
      "Average Time Per Epoch: 31.509821259180704 minutes\n"
     ]
    }
   ],
   "source": [
    "# obtain training time\n",
    "\n",
    "total_time_mins = sum(time_taken) / 60\n",
    "print(f'Total Training Time: {total_time_mins} minutes')\n",
    "\n",
    "avg_time = total_time_mins/10\n",
    "print(f'Average Time Per Epoch: {avg_time} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 loss: 2.16\n",
      "Validation accuracy: 29.36%\n",
      "Time taken for Epoch 11: 2102.367213487625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 loss: 2.14\n",
      "Validation accuracy: 30.91%\n",
      "Time taken for Epoch 12: 2001.2474513053894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 loss: 2.15\n",
      "Validation accuracy: 30.72%\n",
      "Time taken for Epoch 13: 1790.8924679756165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 loss: 2.16\n",
      "Validation accuracy: 28.99%\n",
      "Time taken for Epoch 14: 1764.5591716766357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 loss: 2.13\n",
      "Validation accuracy: 32.33%\n",
      "Time taken for Epoch 15: 1766.3577327728271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 loss: 2.14\n",
      "Validation accuracy: 31.17%\n",
      "Time taken for Epoch 16: 1776.1402904987335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 loss: 2.11\n",
      "Validation accuracy: 34.93%\n",
      "Time taken for Epoch 17: 1766.2503604888916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 loss: 2.11\n",
      "Validation accuracy: 34.61%\n",
      "Time taken for Epoch 18: 1771.2024459838867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 loss: 2.16\n",
      "Validation accuracy: 29.97%\n",
      "Time taken for Epoch 19: 1779.6425502300262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 loss: 2.13\n",
      "Validation accuracy: 32.35%\n",
      "Time taken for Epoch 20: 1758.8219933509827\n",
      "Total training time = 18277.481677770615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████████████████████| 20/20 [04:05<00:00, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.76\n",
      "Test accuracy: 70.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run epochs 11 to 20\n",
    "\n",
    "import time\n",
    "\n",
    "chw = (1, 64, 64) # image dimensions\n",
    "output_dim = 10 # Fashion MNIST has 10 classes\n",
    "n_patches = 16\n",
    "\n",
    "# define augmentation alphas\n",
    "cutmixalpha = 1.0\n",
    "mixupalpha = 0.2\n",
    "\n",
    "# optimal parameters\n",
    "hidden_dim = 8 # number of features in each patch's representation\n",
    "n_encodelayers = 8\n",
    "n_heads = 4 # no of attention heads\n",
    "\n",
    "learning_rate = 0.005\n",
    "batch_size = 512\n",
    "num_epochs = 10\n",
    "\n",
    "# instantiate model continued from previous training\n",
    "if Path('./augmentation_model.pt').exists():\n",
    "    model = torch.load('./augmentation_model.pt')\n",
    "else:\n",
    "    model = VisionTransformer(chw, n_patches, hidden_dim, n_encodelayers, n_heads, output_dim)\n",
    "\n",
    "# instantiate loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# continue training model with training data for another 10 epochs (11 to 20 epochs)\n",
    "time_taken = []\n",
    "for epoch in range(10, 10+num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, epoch, cutmixalpha, mixupalpha)\n",
    "    end_time = time.time()\n",
    "    print(f'Time taken for Epoch {epoch+1}: {end_time - start_time}')\n",
    "    time_taken.append(end_time - start_time)\n",
    "    torch.save(model, f'./augmentation_model.pt')\n",
    "\n",
    "# obtain training time\n",
    "total_time = sum(time_taken)\n",
    "print(f'Total training time = {total_time}')\n",
    "\n",
    "# test model to get test loss and accuracy\n",
    "test_accuracy, test_loss = test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 loss: 2.14\n",
      "Validation accuracy: 31.62%\n",
      "Time taken for Epoch 21: 1519.1007499694824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 loss: 2.13\n",
      "Validation accuracy: 32.81%\n",
      "Time taken for Epoch 22: 1474.4625840187073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 loss: 2.13\n",
      "Validation accuracy: 32.45%\n",
      "Time taken for Epoch 23: 1455.6845943927765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 loss: 2.15\n",
      "Validation accuracy: 30.91%\n",
      "Time taken for Epoch 24: 1443.7331368923187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 loss: 2.12\n",
      "Validation accuracy: 33.97%\n",
      "Time taken for Epoch 25: 1467.1929368972778\n",
      "Total training time = 7360.174002170563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/20 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal training time = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# test model to get test loss and accuracy\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m test_accuracy, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtest_loop\u001b[0;34m(test_loader, model, loss_fn)\u001b[0m\n\u001b[1;32m     36\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_loader))\n\u001b[1;32m     41\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/nndlproj/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nndlproj/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nndlproj/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nndlproj/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "# run epochs 21 to 25 to see if accuracy continues to increase\n",
    "\n",
    "import time\n",
    "\n",
    "chw = (1, 64, 64) # image dimensions\n",
    "output_dim = 10 # Fashion MNIST has 10 classes\n",
    "n_patches = 16\n",
    "\n",
    "# define augmentation alphas\n",
    "cutmixalpha = 1.0\n",
    "mixupalpha = 0.2\n",
    "\n",
    "# optimal parameters\n",
    "hidden_dim = 8 # number of features in each patch's representation\n",
    "n_encodelayers = 8\n",
    "n_heads = 4 # no of attention heads\n",
    "\n",
    "learning_rate = 0.005\n",
    "batch_size = 512\n",
    "num_epochs = 5\n",
    "\n",
    "# instantiate model continued from previous training\n",
    "if Path('./augmentation_model.pt').exists():\n",
    "    model = torch.load('./augmentation_model.pt')\n",
    "else:\n",
    "    model = VisionTransformer(chw, n_patches, hidden_dim, n_encodelayers, n_heads, output_dim)\n",
    "\n",
    "# instantiate loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# continue training model with training data for another 10 epochs (11 to 20 epochs)\n",
    "time_taken = []\n",
    "for epoch in range(20, 20+num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, epoch, cutmixalpha, mixupalpha)\n",
    "    end_time = time.time()\n",
    "    print(f'Time taken for Epoch {epoch+1}: {end_time - start_time}')\n",
    "    time_taken.append(end_time - start_time)\n",
    "    torch.save(model, f'./augmentation_model.pt')\n",
    "\n",
    "# obtain training time\n",
    "total_time = sum(time_taken)\n",
    "print(f'Total training time = {total_time}')\n",
    "\n",
    "# test model to get test loss and accuracy\n",
    "test_accuracy, test_loss = test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 20/20 [02:57<00:00,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.76\n",
      "Test accuracy: 71.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test model to get test loss and accuracy\n",
    "test_accuracy, test_loss = test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
